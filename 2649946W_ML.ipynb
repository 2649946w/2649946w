{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6e9c846-9a5d-4ef7-b720-f1770390092d",
   "metadata": {},
   "source": [
    "Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1973e3-ce1c-4569-ab98-c8506cb1d1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys # importing the package sys which lets you talk to your computer system.\n",
    "\n",
    "assert sys.version_info >= (3, 7) #versions are expressed a pair of numbers (3, 7) which is equivalent to 3.7. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5515a1b-3323-49ea-981b-76d6ce2e982a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from packaging import version #import the package \"version\"\n",
    "import sklearn # import scikit-learn\n",
    "\n",
    "assert version.parse(sklearn.__version__) >= version.parse(\"1.0.1\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d6d5ca-19bd-4f85-b5ab-70bd2ae4b358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rc('font', size=14) #general font size\n",
    "plt.rc('axes', labelsize=14, titlesize=14) #font size for the titles of x and y axes\n",
    "plt.rc('legend', fontsize=14) # font size for legends\n",
    "plt.rc('xtick', labelsize=10) # the font size of labels for intervals marked on the x axis\n",
    "plt.rc('ytick', labelsize=10) # the font size of labels for intervals marked on the y axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196d2580-bbe9-47e9-b5b7-e9ad99546924",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "IMAGES_PATH = Path() / \"images\" / \"classification\"\n",
    "IMAGES_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = IMAGES_PATH / f\"{fig_id}.{fig_extension}\"\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c387bfb-bad7-4e78-b209-36db20884f36",
   "metadata": {},
   "source": [
    "#### Markdown Answers\n",
    "1a.\n",
    "2a.\n",
    "3a.\n",
    "4.a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74405215-42ae-49f8-9f16-93f75352553e",
   "metadata": {},
   "source": [
    "TASK 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de0de65-c05a-443e-b61b-bbace28de949",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "import urllib.request\n",
    "\n",
    "def load_housing_data(): #defines a function that loads the housing data available as .tgz file on a github URL\n",
    "    tarball_path = Path(\"datasets/housing.tgz\") # where you will save your compressed data\n",
    "    if not tarball_path.is_file():\n",
    "        Path(\"datasets\").mkdir(parents=True, exist_ok=True) #create datasets folder if it does not exist\n",
    "        url = \"https://github.com/ageron/data/raw/main/housing.tgz\" # url of where you are getting your data from\n",
    "        urllib.request.urlretrieve(url, tarball_path) # gets the url content and saves it at location specified by tarball_path\n",
    "        with tarfile.open(tarball_path) as housing_tarball: # opens saved compressed file as housing_tarball\n",
    "            housing_tarball.extractall(path=\"datasets\") # extracts the compressed content to datasets folder\n",
    "    return pd.read_csv(Path(\"datasets/housing/housing.csv\")) #uses panadas to read the csv file from the extracted content\n",
    "\n",
    "housing = load_housing_data() #runsthe function defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7d445a-b32e-42f1-bf99-cc350cb05da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "housing = pd.read_csv(Path(\"datasets/housing/housing.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92f5ad1-b65b-48d2-a015-9d764c835efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741b2263-5aa4-4fd0-8d82-9f4df48907fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"ocean_proximity\"].value_counts() # tells you what values the column for `ocean_proximity` can take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0916b5e-cda0-4ca3-b57e-8c5c4e8abe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.hist(bins=50, figsize=(12, 8))\n",
    "save_fig(\"attribute_histogram_plots\")  # extra code\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572f0923-bf79-4728-8dcc-931d467082ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaac539-b95c-46c5-8ea0-23308c9b5800",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import pandas as pd\n",
    "\n",
    "mnist = fetch_openml('mnist_784', as_frame=False, parser='auto')\n",
    "\n",
    "#mnist_dataframe = pd.DataFrame(data=mnist.data, columns=mnist.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0894c5-47c8-483a-9506-11bc9898e5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mnist.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37e06e9-f7d9-492d-ac55-86b414ec360b",
   "metadata": {},
   "source": [
    "Discuss with your peer group what kind of information about housing in a district you think would help predict the median housing price in the district.\n",
    "- Discuss how these decisions might depend on geographical and/or cultural differences and how the information you collect would already bias the data. \n",
    "\n",
    "1a. Regression would be better suited to a machine focused on housing prices as it allows for deviance where as classification as far less wiggle room in that sense.\n",
    "2a. Classification is far more effective for hand written digit as it suits image recognition better\n",
    "3a. Income, Average Age, Average Property Type\n",
    "4a. Place like Glasgow are far more Flat focused whereas the state of Texas focuses on larger houses. This is dute to culture and geo-development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac72a44-b6dc-4cba-8dd7-11db19dc082e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ae70ab-24ad-4fab-9cc4-782b1f75626c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell for python code \n",
    "\n",
    "images = mnist.data\n",
    "categories = mnist.target\n",
    "\n",
    "# insert lines below to print the shape of images and to print the categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4453c3bc-e846-4645-a6fa-cae400213e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extra code to visualise the image of digits\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## the code below defines a function plot_digit. The initial key work `def` stands for define, followed by function name.\n",
    "## the function take one argument image_data in a parenthesis. This is followed by a colon. \n",
    "## Each line below that will be executed when the function is used. \n",
    "## This cell only defines the function. The next cell uses the function.\n",
    "\n",
    "def plot_digit(image_data): # defines a function so that you need not type all the lines below everytime you view an image\n",
    "    image = image_data.reshape(28, 28) #reshapes the data into a 28 x 28 image - before it was a string of 784 numbers\n",
    "    plt.imshow(image, cmap=\"binary\") # show the image in black and white - binary.\n",
    "    plt.axis(\"off\") # ensures no x and y axes are displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d176529d-b652-403b-8094-ee7c3ba25a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise a selected digit with the following code\n",
    "\n",
    "some_digit = mnist.data[4]\n",
    "plot_digit(some_digit)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ac3eef-a528-458e-97d9-c2a237244c20",
   "metadata": {},
   "source": [
    "TASK 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227ffd40-7d29-4049-ba92-a0316d31859c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tratio = 0.2 #to get 20% for testing and 80% for training\n",
    "\n",
    "train_set, test_set = train_test_split(housing, test_size=tratio, random_state=42) \n",
    "## assigning a number to random_state means that everytime you run this you get the same split, unless you change the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e8cbd5-2f9f-4fa0-9d0c-9945cd7eb4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code â€“ shows another way to estimate the probability of bad sample\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sample_size = 1000\n",
    "ratio_female = 0.511\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "samples = (np.random.rand(100_000, sample_size) < ratio_female).sum(axis=1)\n",
    "((samples < 485) | (samples > 535)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2510c71-f976-4dbe-a69e-c4530c5ad6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n",
    "                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
    "                               labels=[1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153d8c77-6723-4339-a3d1-c9cb9dd7313d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tratio = 0.2 #to get 20% for testing and 80% for training\n",
    "\n",
    "strat_train_set, strat_test_set = train_test_split(housing, test_size=tratio, stratify=housing[\"income_cat\"], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30decc96-924d-4a9d-9cd1-b0f2799f153a",
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_test_set[\"income_cat\"].value_counts() / len(strat_test_set) #Prints out in order of the highest proportion first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d0efd3-134b-43c6-96f4-4da32a7989a9",
   "metadata": {},
   "source": [
    "1 Each image is 20x20\n",
    "2 Yes they are justified as they have refined the training data and therefore improved the overall accuracy of the model. As stated, SD-3 was far more usuable and therefore adjusting SD-1 to allow it to have the same kind of readability as SD-3 will improve the dataset overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4012dc65-5df4-4045-aae4-d6827fa1d395",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(mnist.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2c5a87-c6e5-4c8a-8b22-c68b69074dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = mnist.data[:60000]\n",
    "y_train = mnist.target[:60000]\n",
    "\n",
    "X_test = mnist.data[60000:]\n",
    "y_test = mnist.target[60000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22297e23-2f3c-4abc-977e-76af3a3925c7",
   "metadata": {},
   "source": [
    "Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a03a2bc-cf69-4468-97c3-fbd0a920f3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = strat_train_set.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ce301c-cb40-443c-a358-8fdfda96e8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = housing.corr(numeric_only=True) # argument is so that it only calculates for numeric value features\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1489e94-d1ce-4d7a-8454-647fb01970fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "features = [\"median_house_value\", \"median_income\", \"total_rooms\",\n",
    "              \"housing_median_age\"]\n",
    "scatter_matrix(housing[features], figsize=(12, 8))\n",
    "#save_fig(\"scatter_matrix_plot\")  \n",
    "\n",
    "#The line above is extra code you can uncomment (remove the hash at the begining) to save the image.\n",
    "#But, to use this, make sure you ran the code at the beginning of this notebook defining the save_fig function\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51910e19-1dd0-40c8-b2b9-8a41b25ae238",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = strat_train_set.drop(\"median_house_value\", axis=1) ## 1)\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy() ## 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f1bcb3-b05c-4ba2-a0d0-be3363c95bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a9d38d-cd8f-4ecc-8e7e-36d69291c803",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_option3 = housing.copy() #This makes a copy of the data to variable housing_option1, so that we don't mess up the original data.\n",
    "\n",
    "median = housing[\"total_bedrooms\"].median() # calculating mean of the value for total_bedrooms to use in filling missing values\n",
    "housing_option3[\"total_bedrooms\"].fillna(median, inplace=True)  # option 3 - filling missing values with the median\n",
    "\n",
    "housing_option3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2664ed66-8515-47bf-903f-012c30620b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"median\") # initialises the imputer\n",
    "\n",
    "housing_num = housing.select_dtypes(include=[np.number]) ## includes only numeric features in the data\n",
    "\n",
    "imputer.fit(housing_num) #calculates the median for each numeric feature so that the imputer can use them\n",
    "\n",
    "housing_num[:] = imputer.transform(housing_num) # the imputer uses the median to fill the missing values and saves the result in variable X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba50fb4-53ea-40cc-8a61-919b06e5460b",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca7e9d4-5c04-4afe-aa14-b28a2d8b99f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler # get the MinMaxScaler\n",
    "\n",
    "min_max_scaler = MinMaxScaler(feature_range=(-1, 1)) # setup an instance of a scaler\n",
    "housing_num_min_max_scaled = min_max_scaler.fit_transform(housing_num)# use the scaler to transform the data housing_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86aa5d32-d5c8-444d-bc76-2d263da2caa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "housing_num_std_scaled = std_scaler.fit_transform(housing_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1cde77-0a31-4e8c-9294-a1bd95070e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num[:]=std_scaler.fit_transform(housing_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799f8505-2d4c-40eb-8a18-f4633de0ebe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler #This line is not necessary if you ran this prior to running this cell. \n",
    "#We are however including it here for completeness sake.\n",
    "\n",
    "target_scaler = StandardScaler() #instance of Scaler\n",
    "scaled_labels = target_scaler.fit_transform(housing_labels.to_frame()) #calculate the mean and standard deviation and use it to transform the target labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae3be12-bc9f-47eb-bd6c-ff1c73351ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression #get the library from sklearn.linear model\n",
    "\n",
    "model = LinearRegression() #get an instance of the untrained model\n",
    "model.fit(housing_num, scaled_labels)\n",
    "#model.fit(housing[[\"median_income\"]], scaled_labels) #fit it to your data\n",
    "#some_new_data = housing[[\"median_income\"]].iloc[:5]  # pretend this is new data\n",
    "\n",
    "#scaled_predictions = model.predict(some_new_data)\n",
    "#predictions = target_scaler.inverse_transform(scaled_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179cab3e-f840-42bc-adce-72afcceca1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_new_data = housing_num.iloc[:5] #pretend this is new data\n",
    "#some_new_data = housing[[\"median_income\"]].iloc[:5]  # pretend this is new data\n",
    "\n",
    "scaled_predictions = model.predict(some_new_data)\n",
    "predictions = target_scaler.inverse_transform(scaled_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0013546-ed66-435e-81d0-5587ff5d6cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions, housing_labels.iloc[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efcc888-47dc-42ed-ba55-2747f213a237",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rmses = -cross_val_score(model, housing_num, scaled_labels,\n",
    "                              scoring=\"neg_root_mean_squared_error\", cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7f2d9b-8d8e-4a6b-93f8-502f546bd789",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rmses = -cross_val_score(model, housing_num, scaled_labels,\n",
    "                              scoring=\"neg_root_mean_squared_error\", cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2a2c1c-1fdf-44e1-895d-a5db39d2855b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "mnist = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab86d95-9869-4e32-8625-f61e5337017c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(mnist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f48a7a-a20e-4086-b92b-107a4b4e535c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = mnist \n",
    "# (X_train_full, y_train_full) is the 'tuple' related to `a` and (X_test, y_test) is the 'tuple' related to `b`.\n",
    "# X_train_full is the full training data and y_train_full are the corresponding labels \n",
    "# - labels indicate what digit the image is of, for example 5 if it is an image of a handwritten 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a173c6-b5f9-4329-a4d0-a85b0794b4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full = X_train_full / 255.\n",
    "X_test = X_test / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99da4be0-5439-4135-91e5-98604cd5b9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]\n",
    "y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5a99bc-0ce2-44f8-8ed6-9d5ed548c772",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # you won't need to run this line if you ran it before in this notebook. But for completeness.\n",
    "\n",
    "X_train = X_train[..., np.newaxis] #adds a dimension to the image training set - the three dots means keeping everything else the same.\n",
    "X_valid = X_valid[..., np.newaxis]\n",
    "X_test = X_test[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d935ce-10ec-4fc9-a20d-6c0b7e19fa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Unlike scikit-learn, with tensorflow and keras, the model is built by defining each layer of the neural network.\n",
    "# Below, everytime tf.keras.layers is called it is building in another layer\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, kernel_size=3, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Conv2D(64, kernel_size=3, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.25),\n",
    "    tf.keras.layers.Dense(128, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", \n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4097e7e1-473e-43c1-8b38-4ce19cb531cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcc5558-c60b-4021-8cda-b18be23a0171",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import pandas as pd\n",
    "\n",
    "mnist = fetch_openml('mnist_784', as_frame=False, parser='auto')\n",
    "\n",
    "# getting the data and the categories for the data\n",
    "images = mnist.data\n",
    "categories = mnist.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44601d4b-9da3-480d-9037-3cc9c3ae5c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "sgd_clf = SGDClassifier(random_state=42)\n",
    "\n",
    "#cross validation on training data for fit accuracy\n",
    "\n",
    "accuracy = cross_val_score(sgd_clf, images, categories, cv=10)\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe80ffe-f69e-4eda-901f-43ac306e38a5",
   "metadata": {},
   "source": [
    "Task 5: Reflection\r\n",
    "That's it! You've reviewed the machine learning workflow. Before you go, let's reflect on a few things together to fill in the gaps!\r\n",
    "\r\n",
    "Task 5-1: Reflecting on the Machine Learning Workflow\r\n",
    "Get together with your peer group. For the following tasks, you are expected to write a markdown cell describing the workflow required. You are free to include code, but no Python code is required. Discuss the following:\r\n",
    "\r\n",
    "What would you need to do for your code if:\r\n",
    "Your were to use your own data (for example, discuss survey data data and p\n",
    "\n",
    "The data would have to be given a specific and clean path file in order to allow for ease of access. The format of data presentation may have to change, different graphs or visualisation. It would also possibly require the emedding of images within the code so allowance would have to be made for that.\n",
    "hotos)?\r\n",
    "You were \n",
    "changing\r\n",
    "Yo\n",
    "In this regard i found it difficult to predict, i'd have to spend time learning the new model and how to make an effective piece of code within a different sort of framework.\n",
    "ur model?\r\n",
    "Your scali\n",
    "Id probably have to decrease the test and training data sizes in order to allow for learning times to stay reasonable.\n",
    "ng method?\r\n",
    "Your approach to handling mi\n",
    "Id stick with imputing. It is more important to preserve data then wipe it entirely. Whilst it may effect the accuracy of the results it would enable for overall a more precise output.\n",
    "ssing data?\r\n",
    "What is the significance of cross\n",
    "Cross-validation helps to compare and select an appropriate model for the specific predictive modeling problem. It is easy to understand, easy to implement, and tends to have a lower bias than other methods used to count the models efficiency scores\n",
    "umerical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1019fbfe-5d31-400d-8f25-fec449819f2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
